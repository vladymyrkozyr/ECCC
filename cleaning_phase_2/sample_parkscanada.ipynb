{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What this script does:\n",
    "\n",
    "- Remove stopwords: English & French stopwords are loaded from two txt files into two sets. When a comment/post is detected as in French, detectable French stopwords will be removed. Otherwise, English stopwords will be removed accordingly. Note: the stopwords list needs to be updated to suit our purpose, for example, when we need to find out about the attitude/sentiment of comments, we should probably exclude words such as 'couldn't', 'cannot' or 'mustn't' from this list. \n",
    "\n",
    "\n",
    "- Remove non ascii characters: after this step, only digits, English & French characters are kept. Turn it off if it's unnecessary or removes too much implicit information such as emoji's.\n",
    "\n",
    "\n",
    "- Tokenization: with MWETokenizer, multi-word tokens can be added based on our needs, for example, I've added 'climate change', 'canada150', 'justin trudeau' as customized tokens and counted their frequency in later step. \n",
    "\n",
    "\n",
    "- Stemming: stem the tokens obtained from last step.\n",
    "\n",
    "\n",
    "- Output tokens' & stemmers' frequency distribution: output csv files can be found in folder 'word_count', frequency of tokens and stemmers are listed in descending order. The freq_perc column is obtained by dividing the frequency of a word by the total number of comment/posts in each original csv file. Note: Note: there are duplicated tokens in each comment/post, we can choose to count the duplicate or not by commenting/uncommenting a line of code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "#nltk.download()   # comment after first download\n",
    "from nltk.tokenize import sent_tokenize, MWETokenizer, wordpunct_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose a path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../tw/cleaned data/ParksCanada_tweets.csv',\n",
       " '../tw/cleaned data/ParcsCanada_tweets.csv']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#======= Twitter =======\n",
    "rootPath='../tw/cleaned data/*.csv'\n",
    "COLUMN_NAME = 'full_text_cleaned'\n",
    "\n",
    "#======= Facebook =======\n",
    "#rootPath='../fb/cleaned data/comments/*.csv'\n",
    "#COLUMN_NAME = 'comment_message_cleaned'\n",
    "\n",
    "#======= Instagram =======\n",
    "#rootPath='../in/cleaned data/posts/*.csv'\n",
    "#COLUMN_NAME = 'caption'\n",
    "\n",
    "\n",
    "filePaths = glob.glob(rootPath)  \n",
    "display(filePaths)\n",
    "multiWordsPath = './multiwords.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testfile = '../tw/cleaned data/ParksCanada_tweets.csv'\n",
    "filename = os.path.basename(testfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../tw/filtered_data/word_count/'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'../tw/filtered_data/ParksCanada_tweets.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'../tw/filtered_data/word_count/ParksCanada_tweets_word_count.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wordCountOutputDir = os.path.dirname(rootPath).replace('cleaned data', 'filtered_data') + '/word_count/'\n",
    "if not os.path.exists(wordCountOutputDir):\n",
    "    os.makedirs(wordCountOutputDir)\n",
    "cleanedDataFileName = testfile.replace('cleaned data', 'filtered_data')\n",
    "wordCountFileName = wordCountOutputDir + os.path.splitext(filename)[0] + '_word_count.csv'\n",
    "display(wordCountOutputDir)\n",
    "display(cleanedDataFileName)\n",
    "display(wordCountFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWords_en = set(stopwords.words('english'))\n",
    "stopWords_fr = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Adding custermized topic words/tokens to tokenizer...\n",
      "climate_change\n",
      "canada150\n",
      "justin_trudeau\n"
     ]
    }
   ],
   "source": [
    "def detect_lang(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        return 'error'\n",
    "    return lang\n",
    "\n",
    "\n",
    "def filter_stop_words(text):   \n",
    "    stopWords = stopWords_en  \n",
    "    if detect_lang(text) == 'fr':\n",
    "        stopWords = stopWords_fr  \n",
    "    filtered_text = [w for w in wordpunct_tokenize(text) if w.lower() not in stopWords\n",
    "            and len(w) > 1 and w.isalnum()]   \n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "\n",
    "def load_multi_words(filepath):\n",
    "    with open(filepath) as file:\n",
    "        lines = file.readlines()\n",
    "        words = [word.strip() for word in lines]\n",
    "        return words\n",
    "\n",
    "\n",
    "def tokenize_multi_words(topic_list):\n",
    "    result = []\n",
    "    print('>>> Adding custermized topic words/tokens to tokenizer...')\n",
    "    for words in topic_list:\n",
    "        print(words)\n",
    "        result.append(words.split('_'))\n",
    "    return result\n",
    "\n",
    "multiWords = load_multi_words(multiWordsPath)       # load custermized multi-word tokens\n",
    "tokenizedMultiWords = tokenize_multi_words(multiWords)\n",
    "tokenizer = MWETokenizer(tokenizedMultiWords)\n",
    "#tokenizer = MWETokenizer()    # Uncomment this line if no customized multi-word tokens needed\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.tokenize(text.split())   # remove .lower()\n",
    "\n",
    "\n",
    "def stem_text(text):\n",
    "    if detect_lang(text) == 'fr':\n",
    "        stemmer = FrenchStemmer(ignore_stopwords=False)\n",
    "    else:\n",
    "        stemmer = EnglishStemmer(ignore_stopwords=False) \n",
    "    stems = [stemmer.stem(tok) for tok in text]\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load csv files, filter stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(testfile)   # the first unnamed column already exists in csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['text_filtered'] = df[COLUMN_NAME].astype(str).apply(filter_stop_words)\n",
    "#pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load multi-word tokens, tokenize comments/posts, and stem tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Output cleaned data for ParksCanada_tweets.csv\n"
     ]
    }
   ],
   "source": [
    "df['text_tokenized'] = df['text_filtered'].apply(tokenize_text)\n",
    "df['text_stemmed'] = df['text_tokenized'].apply(stem_text)\n",
    "df\n",
    "print('>>> Output cleaned data for ' + filename)\n",
    "df.to_csv(cleanedDataFileName, index=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_lst = df['text_tokenized'].tolist()\n",
    "token_lst\n",
    "token_fdist = FreqDist()\n",
    "for list_i in token_lst:\n",
    "    list_i = set(list_i)  # Adding this line would count a word once even if it appears multple times in one comment/post\n",
    "    for token in list_i:\n",
    "        token_fdist[token.lower()] += 1\n",
    "#token_fdist.most_common(30)\n",
    "#token_fdist['justin_trudeau']       # check the frequency of a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stemmer_lst = df['text_stemmed'].tolist()\n",
    "stemmer_lst\n",
    "stemmer_fdist = FreqDist()\n",
    "for list_i in stemmer_lst:\n",
    "    list_i = set(list_i)  # Adding this line would count a word once even if it appears multple times in one comment/post\n",
    "    for token in list_i:\n",
    "        stemmer_fdist[token.lower()] += 1\n",
    "#stemmer_fdist.most_common(30)\n",
    "display(stemmer_fdist['canada150']) # stemmer doesn't change multi-word tokens\n",
    "display(stemmer_fdist['justin_trudeau'])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output token/stemmer frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Output word frequency distribution for ParksCanada_tweets.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>tok_freq</th>\n",
       "      <th>tok_freq_perc</th>\n",
       "      <th>stemmer</th>\n",
       "      <th>stem_freq</th>\n",
       "      <th>stem_freq_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>us</td>\n",
       "      <td>76</td>\n",
       "      <td>0.097812</td>\n",
       "      <td>park</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.113256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>see</td>\n",
       "      <td>67</td>\n",
       "      <td>0.086229</td>\n",
       "      <td>place</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.101673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>week</td>\n",
       "      <td>61</td>\n",
       "      <td>0.078507</td>\n",
       "      <td>us</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.097812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>places</td>\n",
       "      <td>59</td>\n",
       "      <td>0.075933</td>\n",
       "      <td>see</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.087516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>check</td>\n",
       "      <td>58</td>\n",
       "      <td>0.074646</td>\n",
       "      <td>share</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.083655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>share</td>\n",
       "      <td>56</td>\n",
       "      <td>0.072072</td>\n",
       "      <td>week</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.081081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>get</td>\n",
       "      <td>55</td>\n",
       "      <td>0.070785</td>\n",
       "      <td>site</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.078507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>park</td>\n",
       "      <td>54</td>\n",
       "      <td>0.069498</td>\n",
       "      <td>check</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.077220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>national</td>\n",
       "      <td>53</td>\n",
       "      <td>0.068211</td>\n",
       "      <td>get</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.075933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>canada</td>\n",
       "      <td>51</td>\n",
       "      <td>0.065637</td>\n",
       "      <td>nation</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.072072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>time</td>\n",
       "      <td>47</td>\n",
       "      <td>0.060489</td>\n",
       "      <td>canada</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.065637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nhs</td>\n",
       "      <td>47</td>\n",
       "      <td>0.060489</td>\n",
       "      <td>time</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.061776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>one</td>\n",
       "      <td>45</td>\n",
       "      <td>0.057915</td>\n",
       "      <td>adventur</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.061776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>amp</td>\n",
       "      <td>43</td>\n",
       "      <td>0.055341</td>\n",
       "      <td>nhs</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.060489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>learn</td>\n",
       "      <td>38</td>\n",
       "      <td>0.048906</td>\n",
       "      <td>photo</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.060489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>parks</td>\n",
       "      <td>38</td>\n",
       "      <td>0.048906</td>\n",
       "      <td>year</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.059202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>new</td>\n",
       "      <td>38</td>\n",
       "      <td>0.048906</td>\n",
       "      <td>one</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.059202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sites</td>\n",
       "      <td>37</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>plan</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.056628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>favourite</td>\n",
       "      <td>34</td>\n",
       "      <td>0.043758</td>\n",
       "      <td>amp</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.055341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>show</td>\n",
       "      <td>34</td>\n",
       "      <td>0.043758</td>\n",
       "      <td>learn</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.054054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>year</td>\n",
       "      <td>34</td>\n",
       "      <td>0.043758</td>\n",
       "      <td>new</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.048906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>want</td>\n",
       "      <td>33</td>\n",
       "      <td>0.042471</td>\n",
       "      <td>take</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.048906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>take</td>\n",
       "      <td>33</td>\n",
       "      <td>0.042471</td>\n",
       "      <td>favourit</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.046332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>fun</td>\n",
       "      <td>33</td>\n",
       "      <td>0.042471</td>\n",
       "      <td>make</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.045045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>today</td>\n",
       "      <td>33</td>\n",
       "      <td>0.042471</td>\n",
       "      <td>visit</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.043758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>great</td>\n",
       "      <td>31</td>\n",
       "      <td>0.039897</td>\n",
       "      <td>show</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.043758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>best</td>\n",
       "      <td>31</td>\n",
       "      <td>0.039897</td>\n",
       "      <td>want</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.043758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>make</td>\n",
       "      <td>30</td>\n",
       "      <td>0.038610</td>\n",
       "      <td>start</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.043758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>adventure</td>\n",
       "      <td>29</td>\n",
       "      <td>0.037323</td>\n",
       "      <td>day</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.042471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>day</td>\n",
       "      <td>29</td>\n",
       "      <td>0.037323</td>\n",
       "      <td>fun</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.042471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2451</th>\n",
       "      <td>contemplating</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452</th>\n",
       "      <td>meaning</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>drive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2454</th>\n",
       "      <td>fleet</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>care</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>remembered</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457</th>\n",
       "      <td>rescuing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>1883</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2459</th>\n",
       "      <td>letting</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2460</th>\n",
       "      <td>wearing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>poppy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>poem</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>flanders</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>written</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>fields</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>famous</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>mccrae</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>cecil</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>kinross</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>block</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2471</th>\n",
       "      <td>poppies</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2472</th>\n",
       "      <td>cascade</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>parliament</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2474</th>\n",
       "      <td>maternity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2475</th>\n",
       "      <td>denning</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2476</th>\n",
       "      <td>1917</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>gave</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>fellow</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>robertson</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>canoe</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2481 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              token  tok_freq  tok_freq_perc   stemmer  stem_freq  \\\n",
       "0                us        76       0.097812      park       88.0   \n",
       "1               see        67       0.086229     place       79.0   \n",
       "2              week        61       0.078507        us       76.0   \n",
       "3            places        59       0.075933       see       68.0   \n",
       "4             check        58       0.074646     share       65.0   \n",
       "5             share        56       0.072072      week       63.0   \n",
       "6               get        55       0.070785      site       61.0   \n",
       "7              park        54       0.069498     check       60.0   \n",
       "8          national        53       0.068211       get       59.0   \n",
       "9            canada        51       0.065637    nation       56.0   \n",
       "10             time        47       0.060489    canada       51.0   \n",
       "11              nhs        47       0.060489      time       48.0   \n",
       "12              one        45       0.057915  adventur       48.0   \n",
       "13              amp        43       0.055341       nhs       47.0   \n",
       "14            learn        38       0.048906     photo       47.0   \n",
       "15            parks        38       0.048906      year       46.0   \n",
       "16              new        38       0.048906       one       46.0   \n",
       "17            sites        37       0.047619      plan       44.0   \n",
       "18        favourite        34       0.043758       amp       43.0   \n",
       "19             show        34       0.043758     learn       42.0   \n",
       "20             year        34       0.043758       new       38.0   \n",
       "21             want        33       0.042471      take       38.0   \n",
       "22             take        33       0.042471  favourit       36.0   \n",
       "23              fun        33       0.042471      make       35.0   \n",
       "24            today        33       0.042471     visit       34.0   \n",
       "25            great        31       0.039897      show       34.0   \n",
       "26             best        31       0.039897      want       34.0   \n",
       "27             make        30       0.038610     start       34.0   \n",
       "28        adventure        29       0.037323       day       33.0   \n",
       "29              day        29       0.037323       fun       33.0   \n",
       "...             ...       ...            ...       ...        ...   \n",
       "2451  contemplating         1       0.001287       NaN        NaN   \n",
       "2452        meaning         1       0.001287       NaN        NaN   \n",
       "2453          drive         1       0.001287       NaN        NaN   \n",
       "2454          fleet         1       0.001287       NaN        NaN   \n",
       "2455           care         1       0.001287       NaN        NaN   \n",
       "2456     remembered         1       0.001287       NaN        NaN   \n",
       "2457       rescuing         1       0.001287       NaN        NaN   \n",
       "2458           1883         1       0.001287       NaN        NaN   \n",
       "2459        letting         1       0.001287       NaN        NaN   \n",
       "2460        wearing         1       0.001287       NaN        NaN   \n",
       "2461          poppy         1       0.001287       NaN        NaN   \n",
       "2462           poem         1       0.001287       NaN        NaN   \n",
       "2463       flanders         1       0.001287       NaN        NaN   \n",
       "2464        written         1       0.001287       NaN        NaN   \n",
       "2465         fields         1       0.001287       NaN        NaN   \n",
       "2466         famous         1       0.001287       NaN        NaN   \n",
       "2467         mccrae         1       0.001287       NaN        NaN   \n",
       "2468          cecil         1       0.001287       NaN        NaN   \n",
       "2469        kinross         1       0.001287       NaN        NaN   \n",
       "2470          block         1       0.001287       NaN        NaN   \n",
       "2471        poppies         1       0.001287       NaN        NaN   \n",
       "2472        cascade         1       0.001287       NaN        NaN   \n",
       "2473     parliament         1       0.001287       NaN        NaN   \n",
       "2474      maternity         1       0.001287       NaN        NaN   \n",
       "2475        denning         1       0.001287       NaN        NaN   \n",
       "2476           1917         1       0.001287       NaN        NaN   \n",
       "2477           gave         1       0.001287       NaN        NaN   \n",
       "2478         fellow         1       0.001287       NaN        NaN   \n",
       "2479      robertson         1       0.001287       NaN        NaN   \n",
       "2480          canoe         1       0.001287       NaN        NaN   \n",
       "\n",
       "      stem_freq_perc  \n",
       "0           0.113256  \n",
       "1           0.101673  \n",
       "2           0.097812  \n",
       "3           0.087516  \n",
       "4           0.083655  \n",
       "5           0.081081  \n",
       "6           0.078507  \n",
       "7           0.077220  \n",
       "8           0.075933  \n",
       "9           0.072072  \n",
       "10          0.065637  \n",
       "11          0.061776  \n",
       "12          0.061776  \n",
       "13          0.060489  \n",
       "14          0.060489  \n",
       "15          0.059202  \n",
       "16          0.059202  \n",
       "17          0.056628  \n",
       "18          0.055341  \n",
       "19          0.054054  \n",
       "20          0.048906  \n",
       "21          0.048906  \n",
       "22          0.046332  \n",
       "23          0.045045  \n",
       "24          0.043758  \n",
       "25          0.043758  \n",
       "26          0.043758  \n",
       "27          0.043758  \n",
       "28          0.042471  \n",
       "29          0.042471  \n",
       "...              ...  \n",
       "2451             NaN  \n",
       "2452             NaN  \n",
       "2453             NaN  \n",
       "2454             NaN  \n",
       "2455             NaN  \n",
       "2456             NaN  \n",
       "2457             NaN  \n",
       "2458             NaN  \n",
       "2459             NaN  \n",
       "2460             NaN  \n",
       "2461             NaN  \n",
       "2462             NaN  \n",
       "2463             NaN  \n",
       "2464             NaN  \n",
       "2465             NaN  \n",
       "2466             NaN  \n",
       "2467             NaN  \n",
       "2468             NaN  \n",
       "2469             NaN  \n",
       "2470             NaN  \n",
       "2471             NaN  \n",
       "2472             NaN  \n",
       "2473             NaN  \n",
       "2474             NaN  \n",
       "2475             NaN  \n",
       "2476             NaN  \n",
       "2477             NaN  \n",
       "2478             NaN  \n",
       "2479             NaN  \n",
       "2480             NaN  \n",
       "\n",
       "[2481 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df = pd.DataFrame(list(token_fdist.items()), columns=['token', 'tok_freq'])\n",
    "token_df['tok_freq_perc'] = token_df.tok_freq/len(df)\n",
    "token_df = token_df.sort_values('tok_freq', ascending=False).reset_index(drop=True)\n",
    "\n",
    "stemmer_df = pd.DataFrame(list(stemmer_fdist.items()), columns=['stemmer', 'stem_freq'])\n",
    "stemmer_df['stem_freq_perc'] = stemmer_df.stem_freq/len(df)\n",
    "stemmer_df = stemmer_df.sort_values('stem_freq', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print('>>> Output word frequency distribution for ' + filename)\n",
    "output_df = pd.concat([token_df, stemmer_df], axis=1)\n",
    "output_df.to_csv(wordCountFileName, index=None)   \n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
