{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "#nltk.download()   # comment after first download\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import string\n",
    "from pprint import pprint\n",
    "import logging\n",
    "\n",
    "pd.options.display.max_rows = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set number of topics to classify\n",
    "NUM_TOPICS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list = [social_media_csv_filepath, cleaned_text_column_name, raw_text_column_name]\n",
    "tw_list = ['../tw/filtered_data_spell_corrected/*.csv', 'full_text_cleaned', 'text_original']\n",
    "fb_list = ['../fb/filtered_data_spell_corrected/statuses/*.csv', 'status_message_cleaned', 'text_original']\n",
    "in_list = ['../in/filtered_data_spell_corrected/posts/*.csv', 'caption', 'text_original']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWords_en = set(stopwords.words('english'))\n",
    "stopWords_fr = set(stopwords.words('french'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# read csv files and save targt columns to dataframe\n",
    "def import_csv_columns(list_loc):\n",
    "    filePaths = glob.glob(list_loc[0])  \n",
    "    df_loc = pd.DataFrame(columns=['cleaned_text', 'raw_text'])\n",
    "    for filename in filePaths:\n",
    "        #print(os.path.basename(filename))\n",
    "        df_raw = pd.read_csv(filename)\n",
    "        df_two_col = df_raw[[list_loc[1], list_loc[2]]]\n",
    "        df_two_col.columns = df_loc.columns\n",
    "        #display(len(df_two_col))\n",
    "        df_loc = df_loc.append(df_two_col, ignore_index=True)\n",
    "        #display(len(df_merge))\n",
    "    df_loc = df_loc.dropna(axis=0, how='any')\n",
    "    #print(len(df_loc))\n",
    "    return df_loc\n",
    "\n",
    "\n",
    "def detect_lang(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        return 'error'\n",
    "    return lang\n",
    "\n",
    "\n",
    "def normalize_text(row):   \n",
    "    text = row['cleaned_text']\n",
    "    stopWords = stopWords_en  \n",
    "    if row['lang'] == 'fr':\n",
    "        stopWords = stopWords_fr  \n",
    "    stop_free = ' '.join([w for w in wordpunct_tokenize(text) if w.lower() not in stopWords\n",
    "            and len(w) > 1 and w.isalnum()]) \n",
    "    punc_free = ''.join(ch.lower() for ch in stop_free if ch not in exclude)\n",
    "    normalized = ' '.join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>: Droit devant avec ce mile nautique de plus! ...</td>\n",
       "      <td>RT @GCC_CCG: Droit devant avec ce mile nautiqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>: étro déglaçage dans l'Arctique. Nous fournis...</td>\n",
       "      <td>RT @GCC_CCG: #JeudiRétro déglaçage dans l'Arct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>: Demandez à n'importe quel marin-de bons repa...</td>\n",
       "      <td>RT @GCC_CCG: Demandez à n'importe quel marin-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>: Voyez le NGCC Cape Providence et le NGCC Thu...</td>\n",
       "      <td>RT @GCC_CCG: Voyez le NGCC Cape Providence et ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avis aux utilisateurs de surfaces glacées. égl...</td>\n",
       "      <td>Avis aux utilisateurs de surfaces glacées. #Dé...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nous remercions &amp;amp; offrons nos meilleurs vœ...</td>\n",
       "      <td>Nous remercions &amp;amp; offrons nos meilleurs vœ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>: nouveaux refuges marins au large du Nunavut ...</td>\n",
       "      <td>RT @MPO_DFO: 7 nouveaux refuges marins au larg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>: Avis : se prépare à entamer ses opérations d...</td>\n",
       "      <td>RT @salledepresseGC: Avis : @GCC_CCG se prépar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>: étro le NGCC Terry Fox et son équipage font ...</td>\n",
       "      <td>RT @GCC_CCG: #JeudiRétro le NGCC Terry Fox et ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>: Notre Perlan perché entonne des cantiques av...</td>\n",
       "      <td>RT @MPO_Science: Notre Perlan perché entonne d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92367</th>\n",
       "      <td>Our satellite station in Inuvik receives data ...</td>\n",
       "      <td>Our satellite station in Inuvik receives data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92368</th>\n",
       "      <td>years ago scientists launched Canada’s first s...</td>\n",
       "      <td>#TBT 100 years ago scientists launched Canada’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92369</th>\n",
       "      <td>— here’s a blast from the past! This photo was...</td>\n",
       "      <td>#FlashBackFriday — here’s a blast from the pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92370</th>\n",
       "      <td>We’re helping municipalities improve air, wate...</td>\n",
       "      <td>We’re helping #Ontario municipalities improve ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92371</th>\n",
       "      <td>Natural Resources Canada’s Frank Des Rosiers, ...</td>\n",
       "      <td>Natural Resources Canada’s Frank Des Rosiers, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92372</th>\n",
       "      <td>Alice Wilson, the first female geologist with ...</td>\n",
       "      <td>#TBT Alice Wilson, the first female geologist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92373</th>\n",
       "      <td>Aerial photography records the ever-changing c...</td>\n",
       "      <td>#DYK Aerial photography records the ever-chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92374</th>\n",
       "      <td>We’re investing in electric and alternative fu...</td>\n",
       "      <td>We’re investing in electric and alternative fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92375</th>\n",
       "      <td>Canada is committed to clean energy, innovatio...</td>\n",
       "      <td>Canada is committed to clean energy, innovatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92376</th>\n",
       "      <td>Beautiful scenic view of the east end of Robil...</td>\n",
       "      <td>#TBT Beautiful 1913 scenic view of the east en...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92377 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            cleaned_text  \\\n",
       "0      : Droit devant avec ce mile nautique de plus! ...   \n",
       "1      : étro déglaçage dans l'Arctique. Nous fournis...   \n",
       "2      : Demandez à n'importe quel marin-de bons repa...   \n",
       "3      : Voyez le NGCC Cape Providence et le NGCC Thu...   \n",
       "4      Avis aux utilisateurs de surfaces glacées. égl...   \n",
       "5      Nous remercions &amp; offrons nos meilleurs vœ...   \n",
       "6      : nouveaux refuges marins au large du Nunavut ...   \n",
       "7      : Avis : se prépare à entamer ses opérations d...   \n",
       "8      : étro le NGCC Terry Fox et son équipage font ...   \n",
       "9      : Notre Perlan perché entonne des cantiques av...   \n",
       "...                                                  ...   \n",
       "92367  Our satellite station in Inuvik receives data ...   \n",
       "92368  years ago scientists launched Canada’s first s...   \n",
       "92369  — here’s a blast from the past! This photo was...   \n",
       "92370  We’re helping municipalities improve air, wate...   \n",
       "92371  Natural Resources Canada’s Frank Des Rosiers, ...   \n",
       "92372  Alice Wilson, the first female geologist with ...   \n",
       "92373  Aerial photography records the ever-changing c...   \n",
       "92374  We’re investing in electric and alternative fu...   \n",
       "92375  Canada is committed to clean energy, innovatio...   \n",
       "92376  Beautiful scenic view of the east end of Robil...   \n",
       "\n",
       "                                                raw_text  \n",
       "0      RT @GCC_CCG: Droit devant avec ce mile nautiqu...  \n",
       "1      RT @GCC_CCG: #JeudiRétro déglaçage dans l'Arct...  \n",
       "2      RT @GCC_CCG: Demandez à n'importe quel marin-d...  \n",
       "3      RT @GCC_CCG: Voyez le NGCC Cape Providence et ...  \n",
       "4      Avis aux utilisateurs de surfaces glacées. #Dé...  \n",
       "5      Nous remercions &amp; offrons nos meilleurs vœ...  \n",
       "6      RT @MPO_DFO: 7 nouveaux refuges marins au larg...  \n",
       "7      RT @salledepresseGC: Avis : @GCC_CCG se prépar...  \n",
       "8      RT @GCC_CCG: #JeudiRétro le NGCC Terry Fox et ...  \n",
       "9      RT @MPO_Science: Notre Perlan perché entonne d...  \n",
       "...                                                  ...  \n",
       "92367  Our satellite station in Inuvik receives data ...  \n",
       "92368  #TBT 100 years ago scientists launched Canada’...  \n",
       "92369  #FlashBackFriday — here’s a blast from the pas...  \n",
       "92370  We’re helping #Ontario municipalities improve ...  \n",
       "92371  Natural Resources Canada’s Frank Des Rosiers, ...  \n",
       "92372  #TBT Alice Wilson, the first female geologist ...  \n",
       "92373  #DYK Aerial photography records the ever-chang...  \n",
       "92374  We’re investing in electric and alternative fu...  \n",
       "92375  Canada is committed to clean energy, innovatio...  \n",
       "92376  #TBT Beautiful 1913 scenic view of the east en...  \n",
       "\n",
       "[92377 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine Twitter's tweets, Facebook's posts & Instagram's captions into dataframe 'df_merge'.\n",
    "df_merge = pd.DataFrame(columns=['cleaned_text', 'raw_text'])\n",
    "df_merge = df_merge.append(import_csv_columns(tw_list), ignore_index=True)\n",
    "df_merge = df_merge.append(import_csv_columns(fb_list), ignore_index=True)\n",
    "df_merge = df_merge.append(import_csv_columns(in_list), ignore_index=True)\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lang</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>af</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ca</th>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>45806</td>\n",
       "      <td>45806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>error</th>\n",
       "      <td>118</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fi</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pl</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ro</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sv</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sw</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tl</th>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vi</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zh-cn</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cleaned_text  raw_text\n",
       "lang                         \n",
       "af               25        25\n",
       "ca               55        55\n",
       "cy               10        10\n",
       "da               18        18\n",
       "de               39        39\n",
       "en            45806     45806\n",
       "error           118       118\n",
       "es               20        20\n",
       "et               14        14\n",
       "fi                4         4\n",
       "...             ...       ...\n",
       "pl                3         3\n",
       "pt               13        13\n",
       "ro               30        30\n",
       "so               11        11\n",
       "sv                2         2\n",
       "sw               12        12\n",
       "tl               45        45\n",
       "tr                3         3\n",
       "vi               15        15\n",
       "zh-cn            38        38\n",
       "\n",
       "[28 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detect language of a post. Several languages other than english or french are dectecd but makes up less than 1%\n",
    "df_merge['lang'] = df_merge['cleaned_text'].apply(detect_lang)\n",
    "df_merge.groupby('lang').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out exotic languages\n",
    "df_merge = df_merge[(df_merge['lang'] == 'en') | (df_merge['lang'] == 'fr')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chao/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        [droit, devant, mile, nautique, plus, capitain...\n",
       "1        [étro, déglaçage, arctique, fournissons, servi...\n",
       "2        [demandez, importe, quel, marin, bons, repas, ...\n",
       "3        [voyez, ngcc, cape, providence, ngcc, thunder,...\n",
       "4        [avis, utilisateurs, surface, glacées, églaçag...\n",
       "5        [remercions, amp, offrons, meilleurs, vœux, fi...\n",
       "6        [nouveaux, refuge, marins, large, nunavut, amp...\n",
       "7        [avis, prépare, entamer, opérations, déglaçage...\n",
       "8        [étro, ngcc, terry, fox, équipage, font, escal...\n",
       "9        [perlan, perché, entonne, cantiques, amis, le,...\n",
       "                               ...                        \n",
       "92367    [satellite, station, inuvik, receives, data, r...\n",
       "92368    [year, ago, scientist, launched, canada, first...\n",
       "92369    [blast, past, photo, taken, august, fourth, jo...\n",
       "92370    [helping, municipality, improve, air, water, s...\n",
       "92371    [natural, resource, canada, frank, de, rosiers...\n",
       "92372    [alice, wilson, first, female, geologist, geol...\n",
       "92373    [aerial, photography, record, ever, changing, ...\n",
       "92374    [investing, electric, alternative, fuel, vehic...\n",
       "92375    [canada, committed, clean, energy, innovation,...\n",
       "92376    [beautiful, scenic, view, east, end, robillard...\n",
       "Name: normalized_text, Length: 91778, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-processing text for LDA\n",
    "df_merge['normalized_text'] = df_merge.apply(normalize_text, axis=1)\n",
    "df_merge['normalized_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20948\n",
      "training model for en\n",
      "29631\n",
      "training model for fr\n"
     ]
    }
   ],
   "source": [
    "# train two LDA models, one for 'English', the other for 'French'\n",
    "for lang in ['en', 'fr']:\n",
    "    df_sub = df_merge[df_merge['lang'] == lang]\n",
    "    doc_clean = df_sub['normalized_text'].tolist() \n",
    "    \n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary.save('LDA_dictionary_' + lang + '.dict')  # store the dictionary, for future reference\n",
    "    \n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    corpora.MmCorpus.serialize('LDA_corpus_' + lang + '.mm', corpus)\n",
    "    pprint(len(dictionary.token2id))\n",
    "    \n",
    "    logging.basicConfig(filename='lda_model_' + lang + '.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    \n",
    "    print('training model for ' + lang)\n",
    "    # Running and Training LDA model on the document term matrix.\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary, passes=5)\n",
    "    \n",
    "    # save results\n",
    "    result = ldamodel.show_topics(num_topics=NUM_TOPICS, num_words=100, formatted=False)\n",
    "    df_concat = pd.DataFrame()\n",
    "    for i in range(0, NUM_TOPICS):\n",
    "        df_tmp = pd.DataFrame(result[i][1], columns=['#' + str(result[i][0]) + '_word', '#' + str(result[i][0]) + '_prob'])\n",
    "        df_concat = pd.concat([df_concat, df_tmp], axis=1)\n",
    "        #display(df_concat)\n",
    "    df_concat.to_csv('../LDA_classify_topics_' + lang + '.csv', index=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Experiment on LSI model\n",
    "\n",
    "# extract 3 LSI topics; use the default one-pass algorithm\n",
    "lsi = gensim.models.lsimodel.LsiModel(corpus=corpus, id2word=dictionary, num_topics=NUM_TOPICS)\n",
    "# print the most contributing words (both positively and negatively) for each of the first ten topics\n",
    "lsi.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
