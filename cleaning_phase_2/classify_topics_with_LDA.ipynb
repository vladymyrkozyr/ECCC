{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "#nltk.download()   # comment after first download\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import string\n",
    "from numbers import Number\n",
    "from pprint import pprint\n",
    "import logging\n",
    "\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set number of topics to classify\n",
    "NUM_TOPICS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list = [social_media_csv_filepath, cleaned_text_column_name, raw_text_column_name]\n",
    "tw_list = ['../tw/filtered_data_spell_corrected/*.csv', 'full_text_cleaned', 'text_original']\n",
    "fb_list = ['../fb/filtered_data_spell_corrected/statuses/*.csv', 'status_message_cleaned', 'text_original']\n",
    "in_list = ['../in/filtered_data_spell_corrected/posts/*.csv', 'caption', 'text_original']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords_en = set(stopwords.words('english'))\n",
    "stopWords_fr = set(stopwords.words('french'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# read csv files and save targt columns to dataframe\n",
    "def import_csv_columns(list_loc):\n",
    "    filePaths = glob.glob(list_loc[0])  \n",
    "    df_loc = pd.DataFrame(columns=['cleaned_text', 'raw_text'])\n",
    "    for filename in filePaths:\n",
    "        #print(os.path.basename(filename))\n",
    "        df_raw = pd.read_csv(filename)\n",
    "        df_two_col = df_raw[[list_loc[1], list_loc[2]]]\n",
    "        df_two_col.columns = df_loc.columns\n",
    "        #display(len(df_two_col))\n",
    "        df_loc = df_loc.append(df_two_col, ignore_index=True)\n",
    "        #display(len(df_merge))\n",
    "    df_loc = df_loc.dropna(axis=0, how='any')\n",
    "    #print(len(df_loc))\n",
    "    return df_loc\n",
    "\n",
    "\n",
    "def detect_lang(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        return 'error'\n",
    "    return lang\n",
    "\n",
    "\n",
    "def normalize_text(row):   \n",
    "    stopWords = stopWords_en \n",
    "    if row['lang'] == 'fr':\n",
    "        stopWords = stopWords_fr \n",
    "    text_cols = row['cleaned_text'], row['raw_text']\n",
    "    normalized_text = []\n",
    "    for text in text_cols:\n",
    "        #stop_free = ' '.join([w for w in wordpunct_tokenize(text) if w.lower() not in stopWords\n",
    "        #        and len(w) > 1 and w.isalnum()]) \n",
    "        stop_free = ' '.join([w for w in wordpunct_tokenize(text) if w.lower() not in stopWords and len(w) > 1])\n",
    "        punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "        normalized = ' '.join(lemma.lemmatize(word).lower() for word in punc_free.split() if len(lemma.lemmatize(word)) > 2)\n",
    "        normalized = ' '.join(w for w in normalized.split() if w not in stopWords)\n",
    "        normalized_text.append(normalized.split())\n",
    "    return normalized_text[0], normalized_text[1]\n",
    "\n",
    "\n",
    "def as_percent(text, precision='0.2'):  \n",
    "    if isinstance(text, Number):\n",
    "        return \"{{:{}%}}\".format(precision).format(text)\n",
    "    else:\n",
    "        raise TypeError(\"Numeric type required\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>: Droit devant avec ce mile nautique de plus! ...</td>\n",
       "      <td>RT @GCC_CCG: Droit devant avec ce mile nautiqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>: étro déglaçage dans l'Arctique. Nous fournis...</td>\n",
       "      <td>RT @GCC_CCG: #JeudiRétro déglaçage dans l'Arct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>: Demandez à n'importe quel marin-de bons repa...</td>\n",
       "      <td>RT @GCC_CCG: Demandez à n'importe quel marin-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>: Voyez le NGCC Cape Providence et le NGCC Thu...</td>\n",
       "      <td>RT @GCC_CCG: Voyez le NGCC Cape Providence et ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avis aux utilisateurs de surfaces glacées. égl...</td>\n",
       "      <td>Avis aux utilisateurs de surfaces glacées. #Dé...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92372</th>\n",
       "      <td>Alice Wilson, the first female geologist with ...</td>\n",
       "      <td>#TBT Alice Wilson, the first female geologist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92373</th>\n",
       "      <td>Aerial photography records the ever-changing c...</td>\n",
       "      <td>#DYK Aerial photography records the ever-chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92374</th>\n",
       "      <td>We’re investing in electric and alternative fu...</td>\n",
       "      <td>We’re investing in electric and alternative fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92375</th>\n",
       "      <td>Canada is committed to clean energy, innovatio...</td>\n",
       "      <td>Canada is committed to clean energy, innovatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92376</th>\n",
       "      <td>Beautiful scenic view of the east end of Robil...</td>\n",
       "      <td>#TBT Beautiful 1913 scenic view of the east en...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92377 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            cleaned_text  \\\n",
       "0      : Droit devant avec ce mile nautique de plus! ...   \n",
       "1      : étro déglaçage dans l'Arctique. Nous fournis...   \n",
       "2      : Demandez à n'importe quel marin-de bons repa...   \n",
       "3      : Voyez le NGCC Cape Providence et le NGCC Thu...   \n",
       "4      Avis aux utilisateurs de surfaces glacées. égl...   \n",
       "...                                                  ...   \n",
       "92372  Alice Wilson, the first female geologist with ...   \n",
       "92373  Aerial photography records the ever-changing c...   \n",
       "92374  We’re investing in electric and alternative fu...   \n",
       "92375  Canada is committed to clean energy, innovatio...   \n",
       "92376  Beautiful scenic view of the east end of Robil...   \n",
       "\n",
       "                                                raw_text  \n",
       "0      RT @GCC_CCG: Droit devant avec ce mile nautiqu...  \n",
       "1      RT @GCC_CCG: #JeudiRétro déglaçage dans l'Arct...  \n",
       "2      RT @GCC_CCG: Demandez à n'importe quel marin-d...  \n",
       "3      RT @GCC_CCG: Voyez le NGCC Cape Providence et ...  \n",
       "4      Avis aux utilisateurs de surfaces glacées. #Dé...  \n",
       "...                                                  ...  \n",
       "92372  #TBT Alice Wilson, the first female geologist ...  \n",
       "92373  #DYK Aerial photography records the ever-chang...  \n",
       "92374  We’re investing in electric and alternative fu...  \n",
       "92375  Canada is committed to clean energy, innovatio...  \n",
       "92376  #TBT Beautiful 1913 scenic view of the east en...  \n",
       "\n",
       "[92377 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine Twitter's tweets, Facebook's posts & Instagram's captions into dataframe 'df_merge'.\n",
    "df_merge = pd.DataFrame(columns=['cleaned_text', 'raw_text'])\n",
    "df_merge = df_merge.append(import_csv_columns(tw_list), ignore_index=True)\n",
    "df_merge = df_merge.append(import_csv_columns(fb_list), ignore_index=True)\n",
    "df_merge = df_merge.append(import_csv_columns(in_list), ignore_index=True)\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect language of a post. Several languages other than english or french are dectecd but makes up less than 1%\n",
    "df_merge['lang'] = df_merge['cleaned_text'].apply(detect_lang)\n",
    "#df_merge.groupby('lang').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out exotic languages\n",
    "df_merge = df_merge[(df_merge['lang'] == 'en') | (df_merge['lang'] == 'fr')]\n",
    "#df_backup = df_merge.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chao/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>normalized_cleaned_text</th>\n",
       "      <th>normalized_raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>: Droit devant avec ce mile nautique de plus! ...</td>\n",
       "      <td>RT @GCC_CCG: Droit devant avec ce mile nautiqu...</td>\n",
       "      <td>fr</td>\n",
       "      <td>[droit, devant, mile, nautique, plus, capitain...</td>\n",
       "      <td>[gccccg, droit, devant, mile, nautique, plus, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>: étro déglaçage dans l'Arctique. Nous fournis...</td>\n",
       "      <td>RT @GCC_CCG: #JeudiRétro déglaçage dans l'Arct...</td>\n",
       "      <td>fr</td>\n",
       "      <td>[étro, déglaçage, arctique, fournissons, servi...</td>\n",
       "      <td>[gccccg, jeudirétro, déglaçage, arctique, four...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>: Demandez à n'importe quel marin-de bons repa...</td>\n",
       "      <td>RT @GCC_CCG: Demandez à n'importe quel marin-d...</td>\n",
       "      <td>fr</td>\n",
       "      <td>[demandez, importe, quel, marin, bons, repas, ...</td>\n",
       "      <td>[gccccg, demandez, importe, quel, marin, bons,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>: Voyez le NGCC Cape Providence et le NGCC Thu...</td>\n",
       "      <td>RT @GCC_CCG: Voyez le NGCC Cape Providence et ...</td>\n",
       "      <td>fr</td>\n",
       "      <td>[voyez, ngcc, cape, providence, ngcc, thunder,...</td>\n",
       "      <td>[gccccg, voyez, ngcc, cape, providence, ngcc, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avis aux utilisateurs de surfaces glacées. égl...</td>\n",
       "      <td>Avis aux utilisateurs de surfaces glacées. #Dé...</td>\n",
       "      <td>fr</td>\n",
       "      <td>[avis, utilisateurs, surface, glacées, églaçag...</td>\n",
       "      <td>[avis, utilisateurs, surface, glacées, déglaça...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92372</th>\n",
       "      <td>Alice Wilson, the first female geologist with ...</td>\n",
       "      <td>#TBT Alice Wilson, the first female geologist ...</td>\n",
       "      <td>en</td>\n",
       "      <td>[alice, wilson, first, female, geologist, geol...</td>\n",
       "      <td>[tbt, alice, wilson, first, female, geologist,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92373</th>\n",
       "      <td>Aerial photography records the ever-changing c...</td>\n",
       "      <td>#DYK Aerial photography records the ever-chang...</td>\n",
       "      <td>en</td>\n",
       "      <td>[aerial, photography, record, ever, changing, ...</td>\n",
       "      <td>[dyk, aerial, photography, record, ever, chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92374</th>\n",
       "      <td>We’re investing in electric and alternative fu...</td>\n",
       "      <td>We’re investing in electric and alternative fu...</td>\n",
       "      <td>en</td>\n",
       "      <td>[investing, electric, alternative, fuel, vehic...</td>\n",
       "      <td>[investing, electric, alternative, fuel, vehic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92375</th>\n",
       "      <td>Canada is committed to clean energy, innovatio...</td>\n",
       "      <td>Canada is committed to clean energy, innovatio...</td>\n",
       "      <td>en</td>\n",
       "      <td>[canada, committed, clean, energy, innovation,...</td>\n",
       "      <td>[canada, committed, clean, energy, innovation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92376</th>\n",
       "      <td>Beautiful scenic view of the east end of Robil...</td>\n",
       "      <td>#TBT Beautiful 1913 scenic view of the east en...</td>\n",
       "      <td>en</td>\n",
       "      <td>[beautiful, scenic, view, east, end, robillard...</td>\n",
       "      <td>[tbt, beautiful, 1913, scenic, view, east, end...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91772 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            cleaned_text  \\\n",
       "0      : Droit devant avec ce mile nautique de plus! ...   \n",
       "1      : étro déglaçage dans l'Arctique. Nous fournis...   \n",
       "2      : Demandez à n'importe quel marin-de bons repa...   \n",
       "3      : Voyez le NGCC Cape Providence et le NGCC Thu...   \n",
       "4      Avis aux utilisateurs de surfaces glacées. égl...   \n",
       "...                                                  ...   \n",
       "92372  Alice Wilson, the first female geologist with ...   \n",
       "92373  Aerial photography records the ever-changing c...   \n",
       "92374  We’re investing in electric and alternative fu...   \n",
       "92375  Canada is committed to clean energy, innovatio...   \n",
       "92376  Beautiful scenic view of the east end of Robil...   \n",
       "\n",
       "                                                raw_text lang  \\\n",
       "0      RT @GCC_CCG: Droit devant avec ce mile nautiqu...   fr   \n",
       "1      RT @GCC_CCG: #JeudiRétro déglaçage dans l'Arct...   fr   \n",
       "2      RT @GCC_CCG: Demandez à n'importe quel marin-d...   fr   \n",
       "3      RT @GCC_CCG: Voyez le NGCC Cape Providence et ...   fr   \n",
       "4      Avis aux utilisateurs de surfaces glacées. #Dé...   fr   \n",
       "...                                                  ...  ...   \n",
       "92372  #TBT Alice Wilson, the first female geologist ...   en   \n",
       "92373  #DYK Aerial photography records the ever-chang...   en   \n",
       "92374  We’re investing in electric and alternative fu...   en   \n",
       "92375  Canada is committed to clean energy, innovatio...   en   \n",
       "92376  #TBT Beautiful 1913 scenic view of the east en...   en   \n",
       "\n",
       "                                 normalized_cleaned_text  \\\n",
       "0      [droit, devant, mile, nautique, plus, capitain...   \n",
       "1      [étro, déglaçage, arctique, fournissons, servi...   \n",
       "2      [demandez, importe, quel, marin, bons, repas, ...   \n",
       "3      [voyez, ngcc, cape, providence, ngcc, thunder,...   \n",
       "4      [avis, utilisateurs, surface, glacées, églaçag...   \n",
       "...                                                  ...   \n",
       "92372  [alice, wilson, first, female, geologist, geol...   \n",
       "92373  [aerial, photography, record, ever, changing, ...   \n",
       "92374  [investing, electric, alternative, fuel, vehic...   \n",
       "92375  [canada, committed, clean, energy, innovation,...   \n",
       "92376  [beautiful, scenic, view, east, end, robillard...   \n",
       "\n",
       "                                     normalized_raw_text  \n",
       "0      [gccccg, droit, devant, mile, nautique, plus, ...  \n",
       "1      [gccccg, jeudirétro, déglaçage, arctique, four...  \n",
       "2      [gccccg, demandez, importe, quel, marin, bons,...  \n",
       "3      [gccccg, voyez, ngcc, cape, providence, ngcc, ...  \n",
       "4      [avis, utilisateurs, surface, glacées, déglaça...  \n",
       "...                                                  ...  \n",
       "92372  [tbt, alice, wilson, first, female, geologist,...  \n",
       "92373  [dyk, aerial, photography, record, ever, chang...  \n",
       "92374  [investing, electric, alternative, fuel, vehic...  \n",
       "92375  [canada, committed, clean, energy, innovation,...  \n",
       "92376  [tbt, beautiful, 1913, scenic, view, east, end...  \n",
       "\n",
       "[91772 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-processing text for LDA\n",
    "df_merge['normalized_cleaned_text'], df_merge['normalized_raw_text']  = zip(*df_merge.apply(normalize_text, axis=1))\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merge['normalized_cleaned_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29523\n",
      "training model for cleaned_text in fr\n",
      "74731\n",
      "training model for raw_text in fr\n",
      "21658\n",
      "training model for cleaned_text in en\n",
      "70249\n",
      "training model for raw_text in en\n"
     ]
    }
   ],
   "source": [
    "# train two LDA models, one for 'English', the other for 'French'\n",
    "for lang in ['fr', 'en']:\n",
    "    for col in ['cleaned_text', 'raw_text']:\n",
    "        df_sub = df_merge[df_merge['lang'] == lang]\n",
    "        doc_clean = df_sub['normalized_' + col].tolist() \n",
    "\n",
    "        # Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "        dictionary = corpora.Dictionary(doc_clean)\n",
    "        dictionary.save('./LDA_files/LDA_dictionary_' + col + '_'+ lang + '.dict')  # store the dictionary, for future reference\n",
    "\n",
    "        # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "        corpora.MmCorpus.serialize('./LDA_files/LDA_corpus_' + col + '_' + lang + '.mm', corpus)\n",
    "        pprint(len(dictionary.token2id))\n",
    "\n",
    "        logging.basicConfig(filename='./LDA_files/lda_model_' + col + '_' + lang + '.log',\n",
    "                            format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "        print('training model for ' + col + ' in ' + lang)\n",
    "        # Running and Training LDA model on the document term matrix.\n",
    "        ldamodel = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary, passes=5)\n",
    "\n",
    "        # save results\n",
    "        result = ldamodel.show_topics(num_topics=NUM_TOPICS, num_words=200, formatted=False)\n",
    "        df_concat = pd.DataFrame()\n",
    "        for i in range(0, NUM_TOPICS):\n",
    "            df_tmp = pd.DataFrame(result[i][1], columns=['#' + str(result[i][0]) + '_word', '#' + str(result[i][0]) + '_prob'])\n",
    "            df_concat = pd.concat([df_concat, df_tmp], axis=1)\n",
    "            #display(df_concat)\n",
    "            \n",
    "        df_concat['#0_prob'] = df_concat['#0_prob'].apply(as_percent)\n",
    "        df_concat['#1_prob'] = df_concat['#1_prob'].apply(as_percent)\n",
    "        df_concat['#2_prob'] = df_concat['#2_prob'].apply(as_percent)\n",
    "        \n",
    "        df_concat.to_csv('../LDA_classify_topics_with_' + col + '_' + lang + '.csv', index=None) \n",
    "        df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.960*\"http\" + 0.117*\"canada\" + 0.082*\"amp\" + 0.051*\"learn\" + 0.047*\"help\" + 0.047*\"energy\" + 0.046*\"new\" + 0.039*\"check\" + 0.038*\"cdnag\" + 0.037*\"canadian\"'),\n",
       " (1,\n",
       "  '0.578*\"canada\" + 0.385*\"national\" + 0.341*\"park\" + 0.195*\"photo\" + -0.170*\"http\" + 0.137*\"canada150\" + 0.111*\"parkscanada\" + 0.100*\"site\" + 0.091*\"canadian\" + 0.089*\"climate\"'),\n",
       " (2,\n",
       "  '-0.539*\"photo\" + 0.442*\"canada\" + -0.239*\"park\" + -0.215*\"friday\" + -0.215*\"follower\" + -0.204*\"national\" + 0.115*\"climate\" + -0.112*\"happy\" + -0.111*\"caption\" + -0.110*\"like\"')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Experiment on LSI model\n",
    "\n",
    "# extract 3 LSI topics; use the default one-pass algorithm\n",
    "lsi = gensim.models.lsimodel.LsiModel(corpus=corpus, id2word=dictionary, num_topics=NUM_TOPICS)\n",
    "# print the most contributing words (both positively and negatively) for each of the first ten topics\n",
    "lsi.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
