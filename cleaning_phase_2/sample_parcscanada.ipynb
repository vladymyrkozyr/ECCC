{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What this script does:\n",
    "\n",
    "- Remove stopwords: English & French stopwords are loaded from two txt files into two sets. When a comment/post is detected as in French, detectable French stopwords will be removed. Otherwise, English stopwords will be removed accordingly. Note: the stopwords list needs to be updated to suit our purpose, for example, when we need to find out about the attitude/sentiment of comments, we should probably exclude words such as 'couldn't', 'cannot' or 'mustn't' from this list. \n",
    "\n",
    "\n",
    "- Remove non ascii characters: after this step, only digits, English & French characters are kept. Turn it off if it's unnecessary or removes too much implicit information such as emoji's.\n",
    "\n",
    "\n",
    "- Tokenization: with MWETokenizer, multi-word tokens can be added based on our needs, for example, I've added 'climate change', 'canada150', 'justin trudeau' as customized tokens and counted their frequency in later step. \n",
    "\n",
    "\n",
    "- Stemming: stem the tokens obtained from last step.\n",
    "\n",
    "\n",
    "- Output tokens' & stemmers' frequency distribution: output csv files can be found in folder 'word_count', frequency of tokens and stemmers are listed in descending order. The freq_perc column is obtained by dividing the frequency of a word by the total number of comment/posts in each original csv file. Note: Note: there are duplicated tokens in each comment/post, we can choose to count the duplicate or not by commenting/uncommenting a line of code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import nltk\n",
    "#nltk.download()   # comment after first download\n",
    "from nltk.tokenize import sent_tokenize, MWETokenizer, wordpunct_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose a path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../tw/cleaned data/ParksCanada_tweets.csv',\n",
       " '../tw/cleaned data/ParcsCanada_tweets.csv']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#======= Twitter =======\n",
    "rootPath='../tw/cleaned data/*.csv'\n",
    "COLUMN_NAME = 'full_text_cleaned'\n",
    "\n",
    "#======= Facebook =======\n",
    "#rootPath='../fb/cleaned data/comments/*.csv'\n",
    "#COLUMN_NAME = 'comment_message_cleaned'\n",
    "\n",
    "#======= Instagram =======\n",
    "#rootPath='../in/cleaned data/posts/*.csv'\n",
    "#COLUMN_NAME = 'caption'\n",
    "\n",
    "\n",
    "filePaths = glob.glob(rootPath)  \n",
    "display(filePaths)\n",
    "multiWordsPath = './multiwords.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testfile = '../tw/cleaned data/ParcsCanada_tweets.csv'\n",
    "filename = os.path.basename(testfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../tw/filtered_data/word_count/'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'../tw/filtered_data/ParcsCanada_tweets.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'../tw/filtered_data/word_count/ParcsCanada_tweets_word_count.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wordCountOutputDir = os.path.dirname(rootPath).replace('cleaned data', 'filtered_data') + '/word_count/'\n",
    "if not os.path.exists(wordCountOutputDir):\n",
    "    os.makedirs(wordCountOutputDir)\n",
    "cleanedDataFileName = testfile.replace('cleaned data', 'filtered_data')\n",
    "wordCountFileName = wordCountOutputDir + os.path.splitext(filename)[0] + '_word_count.csv'\n",
    "display(wordCountOutputDir)\n",
    "display(cleanedDataFileName)\n",
    "display(wordCountFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopWords_en = set(stopwords.words('english'))\n",
    "stopWords_fr = set(stopwords.words('french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Adding custermized topic words/tokens to tokenizer...\n",
      "climate_change\n",
      "canada150\n",
      "justin_trudeau\n"
     ]
    }
   ],
   "source": [
    "def detect_lang(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        return 'error'\n",
    "    return lang\n",
    "\n",
    "\n",
    "def filter_stop_words(text):   \n",
    "    stopWords = stopWords_en  \n",
    "    if detect_lang(text) == 'fr':\n",
    "        stopWords = stopWords_fr  \n",
    "    filtered_text = [w for w in wordpunct_tokenize(text) if w.lower() not in stopWords\n",
    "            and len(w) > 1 and w.isalnum()]   \n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "\n",
    "def load_multi_words(filepath):\n",
    "    with open(filepath) as file:\n",
    "        lines = file.readlines()\n",
    "        words = [word.strip() for word in lines]\n",
    "        return words\n",
    "\n",
    "\n",
    "def tokenize_multi_words(topic_list):\n",
    "    result = []\n",
    "    print('>>> Adding custermized topic words/tokens to tokenizer...')\n",
    "    for words in topic_list:\n",
    "        print(words)\n",
    "        result.append(words.split('_'))\n",
    "    return result\n",
    "\n",
    "multiWords = load_multi_words(multiWordsPath)       # load custermized multi-word tokens\n",
    "tokenizedMultiWords = tokenize_multi_words(multiWords)\n",
    "tokenizer = MWETokenizer(tokenizedMultiWords)\n",
    "#tokenizer = MWETokenizer()    # Uncomment this line if no customized multi-word tokens needed\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.tokenize(text.split())   # remove .lower()\n",
    "\n",
    "\n",
    "def stem_text(text):\n",
    "    if detect_lang(text) == 'fr':\n",
    "        stemmer = FrenchStemmer(ignore_stopwords=False)\n",
    "    else:\n",
    "        stemmer = EnglishStemmer(ignore_stopwords=False) \n",
    "    stems = [stemmer.stem(tok) for tok in text]\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load csv files, filter stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(testfile)   # the first unnamed column already exists in csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['text_filtered'] = df[COLUMN_NAME].astype(str).apply(filter_stop_words)\n",
    "#pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load multi-word tokens, tokenize comments/posts, and stem tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Output cleaned data for ParcsCanada_tweets.csv\n"
     ]
    }
   ],
   "source": [
    "df['text_tokenized'] = df['text_filtered'].apply(tokenize_text)\n",
    "df['text_stemmed'] = df['text_tokenized'].apply(stem_text)\n",
    "df\n",
    "print('>>> Output cleaned data for ' + filename)\n",
    "df.to_csv(cleanedDataFileName, index=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_lst = df['text_tokenized'].tolist()\n",
    "token_lst\n",
    "token_fdist = FreqDist()\n",
    "for list_i in token_lst:\n",
    "    list_i = set(list_i)  # Adding this line would count a word once even if it appears multple times in one comment/post\n",
    "    for token in list_i:\n",
    "        token_fdist[token.lower()] += 1\n",
    "#token_fdist.most_common(30)\n",
    "#token_fdist['justin_trudeau']       # check the frequency of a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stemmer_lst = df['text_stemmed'].tolist()\n",
    "stemmer_lst\n",
    "stemmer_fdist = FreqDist()\n",
    "for list_i in stemmer_lst:\n",
    "    list_i = set(list_i)  # Adding this line would count a word once even if it appears multple times in one comment/post\n",
    "    for token in list_i:\n",
    "        stemmer_fdist[token.lower()] += 1\n",
    "#stemmer_fdist.most_common(30)\n",
    "display(stemmer_fdist['canada150']) # stemmer doesn't change multi-word tokens\n",
    "display(stemmer_fdist['justin_trudeau'])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output token/stemmer frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Output word frequency distribution for ParcsCanada_tweets.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>tok_freq</th>\n",
       "      <th>tok_freq_perc</th>\n",
       "      <th>stemmer</th>\n",
       "      <th>stem_freq</th>\n",
       "      <th>stem_freq_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>les</td>\n",
       "      <td>751</td>\n",
       "      <td>0.249834</td>\n",
       "      <td>les</td>\n",
       "      <td>722.0</td>\n",
       "      <td>0.240186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>plus</td>\n",
       "      <td>245</td>\n",
       "      <td>0.081504</td>\n",
       "      <td>parc</td>\n",
       "      <td>319.0</td>\n",
       "      <td>0.106121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cette</td>\n",
       "      <td>220</td>\n",
       "      <td>0.073187</td>\n",
       "      <td>plus</td>\n",
       "      <td>245.0</td>\n",
       "      <td>0.081504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>éfiparcs</td>\n",
       "      <td>204</td>\n",
       "      <td>0.067864</td>\n",
       "      <td>cett</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.072854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>parcs</td>\n",
       "      <td>198</td>\n",
       "      <td>0.065868</td>\n",
       "      <td>éfiparc</td>\n",
       "      <td>205.0</td>\n",
       "      <td>0.068197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>semaine</td>\n",
       "      <td>172</td>\n",
       "      <td>0.057219</td>\n",
       "      <td>endroit</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lhn</td>\n",
       "      <td>171</td>\n",
       "      <td>0.056886</td>\n",
       "      <td>site</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0.064205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>canada</td>\n",
       "      <td>168</td>\n",
       "      <td>0.055888</td>\n",
       "      <td>visit</td>\n",
       "      <td>175.0</td>\n",
       "      <td>0.058217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>écouverte</td>\n",
       "      <td>157</td>\n",
       "      <td>0.052229</td>\n",
       "      <td>semain</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.057552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>découvrez</td>\n",
       "      <td>148</td>\n",
       "      <td>0.049235</td>\n",
       "      <td>lhn</td>\n",
       "      <td>171.0</td>\n",
       "      <td>0.056886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>endroits</td>\n",
       "      <td>144</td>\n",
       "      <td>0.047904</td>\n",
       "      <td>canada</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.055888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lieux</td>\n",
       "      <td>137</td>\n",
       "      <td>0.045576</td>\n",
       "      <td>écouvert</td>\n",
       "      <td>158.0</td>\n",
       "      <td>0.052562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>journée</td>\n",
       "      <td>126</td>\n",
       "      <td>0.041916</td>\n",
       "      <td>découvrez</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.049235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>parc</td>\n",
       "      <td>124</td>\n",
       "      <td>0.041251</td>\n",
       "      <td>historiqu</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0.045908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sites</td>\n",
       "      <td>124</td>\n",
       "      <td>0.041251</td>\n",
       "      <td>lieux</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0.045576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>histoire</td>\n",
       "      <td>110</td>\n",
       "      <td>0.036593</td>\n",
       "      <td>photo</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.045243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>pendant</td>\n",
       "      <td>102</td>\n",
       "      <td>0.033932</td>\n",
       "      <td>journé</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.040585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>année</td>\n",
       "      <td>98</td>\n",
       "      <td>0.032601</td>\n",
       "      <td>fait</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.039587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>nationaux</td>\n",
       "      <td>97</td>\n",
       "      <td>0.032269</td>\n",
       "      <td>histoir</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.038589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>temps</td>\n",
       "      <td>95</td>\n",
       "      <td>0.031603</td>\n",
       "      <td>anné</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.037924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>voici</td>\n",
       "      <td>94</td>\n",
       "      <td>0.031271</td>\n",
       "      <td>nation</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.032934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>où</td>\n",
       "      <td>94</td>\n",
       "      <td>0.031271</td>\n",
       "      <td>pendant</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.032601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>cet</td>\n",
       "      <td>93</td>\n",
       "      <td>0.030938</td>\n",
       "      <td>jour</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.032601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>historiques</td>\n",
       "      <td>92</td>\n",
       "      <td>0.030605</td>\n",
       "      <td>nationaux</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.032269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>national</td>\n",
       "      <td>90</td>\n",
       "      <td>0.029940</td>\n",
       "      <td>temp</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.031603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>visitez</td>\n",
       "      <td>89</td>\n",
       "      <td>0.029607</td>\n",
       "      <td>où</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.031271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>planifiez</td>\n",
       "      <td>86</td>\n",
       "      <td>0.028609</td>\n",
       "      <td>voici</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.031271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>voir</td>\n",
       "      <td>82</td>\n",
       "      <td>0.027279</td>\n",
       "      <td>cet</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.030938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>angl</td>\n",
       "      <td>80</td>\n",
       "      <td>0.026613</td>\n",
       "      <td>an</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.029607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>découvrir</td>\n",
       "      <td>79</td>\n",
       "      <td>0.026281</td>\n",
       "      <td>visitez</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.029607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5310</th>\n",
       "      <td>inspo</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5311</th>\n",
       "      <td>renseignez</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5312</th>\n",
       "      <td>nagent</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5313</th>\n",
       "      <td>lits</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5314</th>\n",
       "      <td>fasciné</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>possédait</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>wapi</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>défu</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>sonne</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>virée</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5320</th>\n",
       "      <td>estime</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5321</th>\n",
       "      <td>maturité</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5322</th>\n",
       "      <td>pleine</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5323</th>\n",
       "      <td>nmca</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5324</th>\n",
       "      <td>mauvais</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5325</th>\n",
       "      <td>nageant</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5326</th>\n",
       "      <td>différents</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5327</th>\n",
       "      <td>taupe</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5328</th>\n",
       "      <td>allé</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5329</th>\n",
       "      <td>hmm</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5330</th>\n",
       "      <td>lait</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331</th>\n",
       "      <td>narrows</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5332</th>\n",
       "      <td>barnaby</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5333</th>\n",
       "      <td>britanniqu</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5334</th>\n",
       "      <td>rive</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5335</th>\n",
       "      <td>planification</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5336</th>\n",
       "      <td>embellira</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5337</th>\n",
       "      <td>manitoba</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5338</th>\n",
       "      <td>esprits</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5339</th>\n",
       "      <td>àrame</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5340 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              token  tok_freq  tok_freq_perc    stemmer  stem_freq  \\\n",
       "0               les       751       0.249834        les      722.0   \n",
       "1              plus       245       0.081504       parc      319.0   \n",
       "2             cette       220       0.073187       plus      245.0   \n",
       "3          éfiparcs       204       0.067864       cett      219.0   \n",
       "4             parcs       198       0.065868    éfiparc      205.0   \n",
       "5           semaine       172       0.057219    endroit      193.0   \n",
       "6               lhn       171       0.056886       site      193.0   \n",
       "7            canada       168       0.055888      visit      175.0   \n",
       "8         écouverte       157       0.052229     semain      173.0   \n",
       "9         découvrez       148       0.049235        lhn      171.0   \n",
       "10         endroits       144       0.047904     canada      168.0   \n",
       "11            lieux       137       0.045576   écouvert      158.0   \n",
       "12          journée       126       0.041916  découvrez      148.0   \n",
       "13             parc       124       0.041251  historiqu      138.0   \n",
       "14            sites       124       0.041251      lieux      137.0   \n",
       "15         histoire       110       0.036593      photo      136.0   \n",
       "16          pendant       102       0.033932     journé      122.0   \n",
       "17            année        98       0.032601       fait      119.0   \n",
       "18        nationaux        97       0.032269    histoir      116.0   \n",
       "19            temps        95       0.031603       anné      114.0   \n",
       "20            voici        94       0.031271     nation       99.0   \n",
       "21               où        94       0.031271    pendant       98.0   \n",
       "22              cet        93       0.030938       jour       98.0   \n",
       "23      historiques        92       0.030605  nationaux       97.0   \n",
       "24         national        90       0.029940       temp       95.0   \n",
       "25          visitez        89       0.029607         où       94.0   \n",
       "26        planifiez        86       0.028609      voici       94.0   \n",
       "27             voir        82       0.027279        cet       93.0   \n",
       "28             angl        80       0.026613         an       89.0   \n",
       "29        découvrir        79       0.026281    visitez       89.0   \n",
       "...             ...       ...            ...        ...        ...   \n",
       "5310          inspo         1       0.000333        NaN        NaN   \n",
       "5311     renseignez         1       0.000333        NaN        NaN   \n",
       "5312         nagent         1       0.000333        NaN        NaN   \n",
       "5313           lits         1       0.000333        NaN        NaN   \n",
       "5314        fasciné         1       0.000333        NaN        NaN   \n",
       "5315      possédait         1       0.000333        NaN        NaN   \n",
       "5316           wapi         1       0.000333        NaN        NaN   \n",
       "5317           défu         1       0.000333        NaN        NaN   \n",
       "5318          sonne         1       0.000333        NaN        NaN   \n",
       "5319          virée         1       0.000333        NaN        NaN   \n",
       "5320         estime         1       0.000333        NaN        NaN   \n",
       "5321       maturité         1       0.000333        NaN        NaN   \n",
       "5322         pleine         1       0.000333        NaN        NaN   \n",
       "5323           nmca         1       0.000333        NaN        NaN   \n",
       "5324        mauvais         1       0.000333        NaN        NaN   \n",
       "5325        nageant         1       0.000333        NaN        NaN   \n",
       "5326     différents         1       0.000333        NaN        NaN   \n",
       "5327          taupe         1       0.000333        NaN        NaN   \n",
       "5328           allé         1       0.000333        NaN        NaN   \n",
       "5329            hmm         1       0.000333        NaN        NaN   \n",
       "5330           lait         1       0.000333        NaN        NaN   \n",
       "5331        narrows         1       0.000333        NaN        NaN   \n",
       "5332        barnaby         1       0.000333        NaN        NaN   \n",
       "5333     britanniqu         1       0.000333        NaN        NaN   \n",
       "5334           rive         1       0.000333        NaN        NaN   \n",
       "5335  planification         1       0.000333        NaN        NaN   \n",
       "5336      embellira         1       0.000333        NaN        NaN   \n",
       "5337       manitoba         1       0.000333        NaN        NaN   \n",
       "5338        esprits         1       0.000333        NaN        NaN   \n",
       "5339          àrame         1       0.000333        NaN        NaN   \n",
       "\n",
       "      stem_freq_perc  \n",
       "0           0.240186  \n",
       "1           0.106121  \n",
       "2           0.081504  \n",
       "3           0.072854  \n",
       "4           0.068197  \n",
       "5           0.064205  \n",
       "6           0.064205  \n",
       "7           0.058217  \n",
       "8           0.057552  \n",
       "9           0.056886  \n",
       "10          0.055888  \n",
       "11          0.052562  \n",
       "12          0.049235  \n",
       "13          0.045908  \n",
       "14          0.045576  \n",
       "15          0.045243  \n",
       "16          0.040585  \n",
       "17          0.039587  \n",
       "18          0.038589  \n",
       "19          0.037924  \n",
       "20          0.032934  \n",
       "21          0.032601  \n",
       "22          0.032601  \n",
       "23          0.032269  \n",
       "24          0.031603  \n",
       "25          0.031271  \n",
       "26          0.031271  \n",
       "27          0.030938  \n",
       "28          0.029607  \n",
       "29          0.029607  \n",
       "...              ...  \n",
       "5310             NaN  \n",
       "5311             NaN  \n",
       "5312             NaN  \n",
       "5313             NaN  \n",
       "5314             NaN  \n",
       "5315             NaN  \n",
       "5316             NaN  \n",
       "5317             NaN  \n",
       "5318             NaN  \n",
       "5319             NaN  \n",
       "5320             NaN  \n",
       "5321             NaN  \n",
       "5322             NaN  \n",
       "5323             NaN  \n",
       "5324             NaN  \n",
       "5325             NaN  \n",
       "5326             NaN  \n",
       "5327             NaN  \n",
       "5328             NaN  \n",
       "5329             NaN  \n",
       "5330             NaN  \n",
       "5331             NaN  \n",
       "5332             NaN  \n",
       "5333             NaN  \n",
       "5334             NaN  \n",
       "5335             NaN  \n",
       "5336             NaN  \n",
       "5337             NaN  \n",
       "5338             NaN  \n",
       "5339             NaN  \n",
       "\n",
       "[5340 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df = pd.DataFrame(list(token_fdist.items()), columns=['token', 'tok_freq'])\n",
    "token_df['tok_freq_perc'] = token_df.tok_freq/len(df)\n",
    "token_df = token_df.sort_values('tok_freq', ascending=False).reset_index(drop=True)\n",
    "\n",
    "stemmer_df = pd.DataFrame(list(stemmer_fdist.items()), columns=['stemmer', 'stem_freq'])\n",
    "stemmer_df['stem_freq_perc'] = stemmer_df.stem_freq/len(df)\n",
    "stemmer_df = stemmer_df.sort_values('stem_freq', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print('>>> Output word frequency distribution for ' + filename)\n",
    "output_df = pd.concat([token_df, stemmer_df], axis=1)\n",
    "output_df.to_csv(wordCountFileName, index=None)   \n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
